{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1578,
     "status": "ok",
     "timestamp": 1629767569047,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "hIYqysnLS35n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from hyperopt import hp, tpe, Trials, STATUS_OK, fmin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge,ARDRegression,BayesianRidge,ElasticNet,HuberRegressor\n",
    "from sklearn.linear_model import Lasso, LassoLars, LinearRegression, LogisticRegression, PassiveAggressiveRegressor,Ridge,SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1629767596615,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "LQ7rfpRNS35p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class morgan_fp:\n",
    "    def __init__(self, radius, length):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "    def __call__(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        fp = AllChem.GetHashedMorganFingerprint(mol, self.radius, self.length)\n",
    "        npfp = np.array(list(fp)).astype('float32')\n",
    "        return npfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_data_pd(data,fp):\n",
    "    data['c-fp'] = data['new_cation'].apply(fp)\n",
    "    x_c=np.array(list(data['c-fp']))\n",
    "    x_c_pd = pd.DataFrame(data=x_c, columns=[f'c_fp_{i}' for i in range(x_c.shape[1])])\n",
    "    \n",
    "    data['a-fp'] = data['new_anion'].apply(fp)\n",
    "    x_a=np.array(list(data['a-fp']))\n",
    "    x_a_pd = pd.DataFrame(data=x_a, columns=[f'a_fp_{i}' for i in range(x_a.shape[1])])\n",
    "    \n",
    "    hh_fp = pd.concat([x_c_pd, x_a_pd], axis =1)\n",
    "    \n",
    "    x_con_pd = data[['T', 'P']]\n",
    "    \n",
    "    hh_final = pd.concat([hh_fp,x_con_pd], axis=1)\n",
    "    y = data['CO2-exp'].values\n",
    "    \n",
    "    return hh_final, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575487,
     "status": "ok",
     "timestamp": 1629768374599,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "bVkf6pFKS351",
    "outputId": "67052304-88d8-4947-9d2e-2fcdea053368",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp = morgan_fp(1, 1024)\n",
    "models = [CatBoostRegressor(verbose =False,random_state=10),XGBRegressor(random_state=10), svm.SVR(),RandomForestRegressor(random_state=10),\n",
    "          AdaBoostRegressor(random_state=10),BaggingRegressor(random_state=10),GradientBoostingRegressor(random_state=10),\n",
    "         Lasso(), Ridge(random_state=10)]\n",
    "results = pd.DataFrame(columns=['train_rmse','train_r2', 'test_rmse','test_r2', 'name'])\n",
    "k =0 \n",
    "for model in models:\n",
    "    val_loss=[]\n",
    "    train_loss=[]\n",
    "    val_r2 = []\n",
    "    train_r2=[]\n",
    "    for i in range(1, 6):\n",
    "        traindata = pd.read_csv(f'data/train_{i}_group_co2.csv')\n",
    "        valdata = pd.read_csv(f'data/val_{i}_group_co2.csv')\n",
    "        x_train_pd, y_train, =conv_data_pd(traindata, fp)\n",
    "        x_train= x_train_pd.values\n",
    "        x_val_pd, y_val, =conv_data_pd(valdata,fp)\n",
    "        x_val=x_val_pd.values\n",
    "        model.fit(x_train, y_train)\n",
    "        y_val_pred =model.predict(x_val)\n",
    "        y_train_pred =model.predict(x_train)\n",
    "        train_loss.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "        val_loss.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "        train_r2.append(r2_score(y_train, y_train_pred))\n",
    "        val_r2.append(r2_score(y_val, y_val_pred))\n",
    "    results.loc[k, 'train_rmse']=np.mean(train_loss)\n",
    "    results.loc[k, 'test_rmse']=np.mean(val_loss)+np.std(val_loss)\n",
    "    results.loc[k, 'train_r2']=np.mean(train_r2)\n",
    "    results.loc[k, 'test_r2']=np.mean(val_r2)+np.std(val_r2)\n",
    "    results.loc[k, 'name']=model.__class__.__name__\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1629768431812,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "KNpFk3R4S352",
    "outputId": "6cd81b6c-0b23-452b-f7da-9fa17c14d9e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.sort_values(['test_rmse'], ascending= True, inplace = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "space = {'depth': hp.quniform('depth', 1,6,1),\n",
    "         'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 100.0),\n",
    "          'learning_rate':hp.loguniform('learning_rate', np.log(0.0001), np.log(0.025)),\n",
    "          'iterations':hp.quniform('iterations', 1, 1000, 1),\n",
    "         'bagging_temperature':hp.uniform('bagging_temperature', 1, 200),\n",
    "         'random_strength':hp.uniform('random_strength', 1, 200)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1629768605306,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "rnFNP11XS353",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(params):\n",
    "    #fp = morgan_fp(params['fp_radius'], params['fp_length'])\n",
    "    model = CatBoostRegressor(depth = params['depth'], l2_leaf_reg= params['l2_leaf_reg'], learning_rate = params['learning_rate'],\n",
    "                         iterations=params['iterations'], bagging_temperature=params['bagging_temperature'],\n",
    "                         random_strength=params['random_strength'],random_state=10, verbose=False)\n",
    "    val_loss=[]\n",
    "    train_loss=[]\n",
    "    for i in range(1, 6):\n",
    "        traindata = pd.read_csv(f'data/train_{i}_group_co2.csv')\n",
    "        valdata = pd.read_csv(f'data/val_{i}_group_co2.csv')\n",
    "        x_train, y_train, =conv_data_pd(traindata,fp)\n",
    "        x_val, y_val, =conv_data_pd(valdata,fp)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_val_pred =model.predict(x_val)\n",
    "        y_train_pred =model.predict(x_train)\n",
    "        train_loss.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "        val_loss.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "    return np.mean(val_loss)+np.std(val_loss), np.mean(train_loss)\n",
    "\n",
    "def objective(params):\n",
    "    global ITERATION\n",
    "    ITERATION +=1\n",
    "    for name in ['depth', 'iterations']:\n",
    "        params[name] = int(params[name])\n",
    "    loss, train_loss = fit(params)\n",
    "    loss =loss\n",
    "    off_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(off_connection)\n",
    "    writer.writerow([loss,train_loss, params, ITERATION])\n",
    "    #pickle.dump(bayes_trial, open(dir_data + \"h2_cat.p\", \"wb\"))\n",
    "    return {'loss':loss,'train_loss':train_loss, 'params': params, 'iteration':ITERATION, 'status':STATUS_OK}\n",
    "\n",
    "import csv\n",
    "out_file ='data/co2_MF_hyper.csv'\n",
    "off_connection =open( out_file, 'w')\n",
    "writer = csv.writer(off_connection)\n",
    "writer.writerow(['loss','train_loss', 'params', 'iteration'])\n",
    "off_connection.close()\n",
    "\n",
    "tpe_algo = tpe.suggest\n",
    "bayes_trial = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFSA6HJZS354",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "global ITERATION\n",
    "ITERATION =0\n",
    "best = fmin(fn = objective, space =space, algo = tpe_algo, trials = bayes_trial, \n",
    "            early_stop_fn=no_progress_loss(100),max_evals=3000, rstate= np.random.default_rng()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1629736672479,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "MUiIBkZtS355",
    "outputId": "03db6433-ea89-4b61-b69e-5d049421d5f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv('data/co2_MF_hyper.csv')\n",
    "result.sort_values('loss', ascending= True, inplace = True)\n",
    "result.reset_index(drop = True, inplace =True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1629737032693,
     "user": {
      "displayName": "Shifa Zhong",
      "photoUrl": "",
      "userId": "10488948237386234172"
     },
     "user_tz": 240
    },
    "id": "mzIjyoWrS357",
    "outputId": "1dd2e6a0-9c11-4057-85ab-3b4544f96599",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "params = ast.literal_eval(result['params'][0])                                                                                                                                                                                                                                                               \n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dZaJpXYS357",
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdata = pd.read_csv(f'data/test_group_co2.csv')\n",
    "testdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(f'data/train_{1}_group_co2.csv')\n",
    "valdata = pd.read_csv(f'data/val_{1}_group_co2.csv')\n",
    "train_data_merge= pd.concat([traindata, valdata],ignore_index=True)\n",
    "train_data_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A615GI7hS35_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fp = morgan_fp(params['fp_radius'], params['fp_length'])\n",
    "model = CatBoostRegressor(depth = params['depth'], l2_leaf_reg= params['l2_leaf_reg'],learning_rate= params['learning_rate'],\n",
    "                         iterations=params['iterations'], bagging_temperature=params['bagging_temperature'],\n",
    "                         random_strength=params['random_strength'],random_state=10, verbose=False)\n",
    "\n",
    "x_test, y_test =conv_data_pd(testdata, fp)\n",
    "x_train, y_train =conv_data_pd(train_data_merge, fp)\n",
    "model.fit(x_train, y_train)\n",
    "y_test_pred =model.predict(x_test)\n",
    "y_train_pred =model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ref_MF.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "co2",
   "language": "python",
   "name": "co2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
